{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "**Disclaimers**\n",
    "- Lesson will focus on classical neural networks with occasional hints towards more current (but less utilized) techniques\n",
    "- The math is going to be in the background\n",
    "- Neural networks is an evolving field\n",
    "- Most data science jobs come nowhere near pushing the limits of neural networks\n",
    "- There is still some controversy surrounding NNs, especially from more classical statisticians\n",
    "\n",
    "## The brain & neural networks\n",
    "**Note:** Biological explanations may be simplified, but simplifications are perpetuated in the ML algorithms\n",
    "- The brain & nervous system is made up of neurons and axons\n",
    "- **Neurons**\n",
    "    - Information processors of the brain\n",
    "    - React to signals and create responses\n",
    "- **Axons**\n",
    "    - Connect neurons to each other\n",
    "    - Human brain is made of billions of neurons connected through a web of axons\n",
    "- Information passes through several neurons in response to input or stimulus\n",
    "- ML NN algorithm is also referred to as artificial neural network to differentiate from the biological NN\n",
    "\n",
    "## Artificial neural networks\n",
    "- Recall the perceptron model\n",
    "    - Takes in some data and generates a response\n",
    "    - Usually performs poorly\n",
    "- Ensemble modeling concepts to build NN model\n",
    "    - Bagging: have many perceptrons in the model (similar to decision trees in random forest)\n",
    "    - Boosting: feed output from one set of perceptrons as inputs to the next\n",
    "- **Layers:** the columns of perceptrons, can be single layer or multilayered\n",
    "    - **Visible layer:** first layer\n",
    "    - **Hidden layer:** layers after the first layer\n",
    "        - Features built on features\n",
    "        - No direct observation of their inputs or outputs\n",
    "- Perceptrons differ from one another\n",
    "    - Different perceptrons give different variables different weights to ensure they're all not doing the same thing\n",
    "    - When done on a large scale, they overcome the initial assumption of linearity\n",
    "- Fully connected networks are ones where every perceptron in one layer links to every perceptron in the following layer\n",
    "- Can weight perceptrons so they are not fully connected or evenly balanced in their connections\n",
    "- Types of neural networks:\n",
    "    - Single layer of perceptrons is just a simple, single layer neural network\n",
    "    - Many-layered networks enter into the deep learning realm\n",
    "    - The way layers feed back into each other is a key differentiator between different styles of neural networks\n",
    "    - Data does not always flow in one direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
