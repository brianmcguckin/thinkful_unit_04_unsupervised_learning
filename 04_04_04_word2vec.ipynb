{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "- Most common unsupervised NN approach for NLP\n",
    "- Shallow NN model for converting words to vectors using distributed representation\n",
    "    - Each word represented by many neurons\n",
    "    - Each neuron involved in representing many words\n",
    "- Works by assigning vectors of random values to each word\n",
    "    - For word W, looks at words near W in sentence\n",
    "    - Shifts values in word vectors such that vectors for words near W are closer to W vector\n",
    "    - Words not near W are shifted further away from W vector\n",
    "    - Eventually results in words that often appear together having vectors near one another\n",
    "    - Words that rarely/never appear together have vectors far away from one another\n",
    "    - Similarity scores can then be computed for each word pair by taking cosine of the vectors\n",
    "- Difference from Latent Semantic Analysis\n",
    "    - LSA creates vector representations of sentences based on the words in them\n",
    "    - word2vec creates representations of individual words based on the words around them\n",
    "- Useful for parsing requests written by humans\n",
    "    - Humans can communicate the same concept many different ways\n",
    "    - Humans know silverware and utensils can refer to the same thing but computers do not\n",
    "    - word2vec helps computers infer meaning by  looking at words with close vectors\n",
    "    - Search engines: return best results for query and not just ones containing exact words used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating vectors\n",
    "- **Continuous Bag of Words (CBOW):** identity of a word is predicted using the words near it in a sentence\n",
    "- **Skip-gram:** identities of words are predicted from the word they surround\n",
    "    - Seems to work better for larger corpuses\n",
    "- Example: 'Terry Gilliam is a better comedian than a director'\n",
    "    - CBOW will try to predict 'comedian' using is a better than a director, vector for comedian pulled closer to other words\n",
    "    - Skip-gram will try to predict is a better than a director using 'comedian', vectors for other words pulled closer to 'comedian'\n",
    "- Each time a word is processed some vectors are moved further away\n",
    "    - Negative sampling: each time a word is pulled toward neighbors, others are pushed away\n",
    "    - Hierarchical softmax: every neighboring word is pulled closer or farther from a subset of words chose based on a tree of probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "- word2vec operates on the assumption that frequent proximity indicates similarity, but words can similar in various ways\n",
    "- Can identify similarities between words that never occur near one another in the corpus\n",
    "- Vectors can be used to convert analogies into mathematical expressions\n",
    "    - king:queen :: man:woman\n",
    "    - king + woman - man = queen\n",
    "- Works best on very a large corpus (billions of words)\n",
    "- Example only has 2 million words so results will not be great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaner function\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--', ' ',text)\n",
    "    text = re.sub('[\\[].*?[\\]]','',text)\n",
    "    text = re.sub(r'Chapter \\d+','', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "#import all austen in gutenberg corpus\n",
    "#austen = ''\n",
    "#for novel in ['persuasion','emma','sense']:\n",
    "#    work = gutenberg.raw('austen-' + novel + '.txt')\n",
    "#    austen = austen + work\n",
    "p = gutenberg.raw('austen-persuasion.txt')\n",
    "e = gutenberg.raw('austen-emma.txt')\n",
    "s = gutenberg.raw('austen-sense.txt')\n",
    "\n",
    "p_clean = text_cleaner(p)\n",
    "e_clean = text_cleaner(e)\n",
    "s_clean = text_cleaner(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "p_doc = nlp(p_clean)\n",
    "e_doc = nlp(e_clean)\n",
    "s_doc = nlp(s_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'daughter', 'eld', 'give', 'thing', 'tempt']\n",
      "we have 17853 sentences and 2006270 tokens.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in p_doc.sents:\n",
    "    sentence = [\n",
    "        token.lemma_.lower()\n",
    "        for token in sentence\n",
    "        if not token.is_stop\n",
    "        and not token.is_punct\n",
    "    ]\n",
    "    sentences.append(sentence)\n",
    "\n",
    "\n",
    "for sentence in e_doc.sents:\n",
    "    sentence = [\n",
    "        token.lemma_.lower()\n",
    "        for token in sentence\n",
    "        if not token.is_stop\n",
    "        and not token.is_punct\n",
    "    ]\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "for sentence in s_doc.sents:\n",
    "    sentence = [\n",
    "        token.lemma_.lower()\n",
    "        for token in sentence\n",
    "        if not token.is_stop\n",
    "        and not token.is_punct\n",
    "    ]\n",
    "    sentences.append(sentence)\n",
    "\n",
    "print(sentences[20])\n",
    "print('we have {} sentences and {} tokens.'.format(len(sentences), (len(p_clean)+len(e_clean)+len(s_clean))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.944455000000005\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import time\n",
    "\n",
    "start_time = time.clock()\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers=2,\n",
    "    min_count=10,\n",
    "    window=6,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=300,\n",
    "    hs=1)\n",
    "\n",
    "print(time.clock() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('people', 0.578013002872467), ('friend', 0.47928112745285034), ('introduction', 0.4748184084892273), ('daughter', 0.46245941519737244), ('monstrous', 0.4612240195274353), ('visit', 0.44612759351730347), ('anne', 0.43285226821899414), ('way', 0.4328392744064331), ('choice', 0.4274982213973999), ('stranger', 0.4229855239391327)]\n",
      "0.72369635\n",
      "0.10934523\n",
      "marriage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "vocab = model.wv.vocab.keys()\n",
    "\n",
    "print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "\n",
    "print(model.wv.similarity('loud','aloud'))\n",
    "print(model.wv.similarity('mr','mrs'))\n",
    "\n",
    "print(model.doesnt_match('breakfast marriage dinner lunch'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 0\n",
    "Play with word2vec hyperparameters and see if you can improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.520030999999989\n",
      "[('disinclined', 0.8604533672332764), ('entry', 0.8412480354309082), ('inexperienc', 0.8180102109909058), ('snatch', 0.8139647245407104), ('swisserland', 0.8108318448066711), ('inforc', 0.8068943023681641), ('parson', 0.7885650396347046), ('proficient', 0.7775000333786011), ('enchanting', 0.7773404121398926), ('brood', 0.7767763137817383)]\n",
      "0.6061417\n",
      "0.5211655\n",
      "marriage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers=2,\n",
    "    min_count=1,\n",
    "    window=1,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=300,\n",
    "    hs=1)\n",
    "\n",
    "print(time.clock() - start_time)\n",
    "\n",
    "vocab = model.wv.vocab.keys()\n",
    "\n",
    "print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "\n",
    "print(model.wv.similarity('loud','aloud'))\n",
    "print(model.wv.similarity('mr','mrs'))\n",
    "\n",
    "print(model.doesnt_match('breakfast marriage dinner lunch'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 1\n",
    "As we mentioned, word2vec really works best on a big corpus, but it can take half a day to clean such a corpus and run word2vec on it.  Fortunately, there are word2vec models available that have already been trained on _really_ big corpora. They are big files, but you can download a [pretrained model of your choice here](https://github.com/3Top/word2vec-api). At minimum, the ones built with word2vec (check the \"Architecture\" column) should load smoothly using an appropriately modified version of the code below, and you can play to your heart's content.\n",
    "\n",
    "Because the models are so large, however, you may run into memory problems or crash the kernel. If you can't get a pretrained model to run locally, check out this [interactive web app of the Google News model](https://rare-technologies.com/word2vec-tutorial/#bonus_app) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seriously the code for this nlp unit has been a complete disaster and i'm done troubleshooting it\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format ('./model/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
