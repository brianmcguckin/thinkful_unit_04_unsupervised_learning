{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised NLP\n",
    "\n",
    "### Semantics\n",
    "- Supervised learning: knows often used words, parts of speech, but not the meanings of words\n",
    "    - Couldn't say whether 'lady' is more similar to 'queen' or 'car'\n",
    "    - Synonyms become problematic\n",
    "    - Connotative information is lost\n",
    "- Not having semantic information limits NLP applicability\n",
    "- Feeding this explicitly to supervised models is basically impossible\n",
    "- Unsupervised learning: NNs in particular \n",
    "    - Can learn these rules through pattern identification\n",
    "    - Use trained unsupervised model to understand new sentences\n",
    "    - Limited by corpus (just like supervised learners); a model trained on novels will be poor at medical text\n",
    "    - Latent Semantic Analysis\n",
    "\n",
    "### Converting sentences to vectors\n",
    "- Term-document matrix: column for each sentence (document) and row for each word\n",
    "    - 'Document' is a flexible term to mean individual chunks of text\n",
    "    - Can be sentences, paragraphs, entire files, etc\n",
    "- Weighting: words that occur in many different sentences weighted lower than those that occur in fewer sentences\n",
    "\n",
    "### Quantifying documents\n",
    "- **Document frequency (*df*):** how many documents a word appears in\n",
    "- **Collection frequency (*cf*):** how often a word appears total over all documents\n",
    "\n",
    "### Penalizing indiscriminate words\n",
    "- **Inverse document frequency (*idf*):**\n",
    "    - Calculate ratio of total docs (*N*) divided by *df*, take log (base 2) to get *idf* for each term (*t*)\n",
    "    - $ idf_t = log\\dfrac N{df_t}$\n",
    "    \n",
    "### Term-frequency weights\n",
    "- Weights for how frequently a term appears within a document\n",
    "    - $tf - idf_{t,d} = (tf_{t,d})(idf_t)$\n",
    "    - More frequent gets higher weights for that document\n",
    "    - tf_idf score highest for a term occuring a lot within a small number of documents\n",
    "    - tf_idf score lowest for a term occuring in most or all sentences\n",
    "- 'Translation' from human-readable language to computer-usable numeric form\n",
    "- Some information loss is inevitable\n",
    "- Decision-points during translation step\n",
    "    - Which stop words to include or exclude\n",
    "    - Whether to use phrases as terms ('Monty Python' instead of 'Monty' and 'Python')\n",
    "    - Threshold for infrequent words (longer documents may warrant higher thresholds)\n",
    "    - How many terms to keep (longer documents may end up creating very long vectors)\n",
    "\n",
    "## Vector Space Model\n",
    "- **VSM:** the vector representation of the text\n",
    "- Used to compute the similarity between known documents and new ones\n",
    "    - Search engines use to match query to possible results\n",
    "- Sentences exist in *n*-dimensional space where *n* is equal to the number of terms in term-doc matrix\n",
    "- Computing similarity:\n",
    "    - Transform new document into a vector and place in this space\n",
    "    - Calculate how different the angles are for original vector and new vector\n",
    "    - Identify vector whose angle is closest to the new vector angle\n",
    "        - Typically cosine of the angle between the vectors\n",
    "        - Identical vectors: angle = 0 and cosine = 1\n",
    "        - Orthogonal vectors: angle = 90 and cosine = 0\n",
    "        - Higher cosine = higher similarity\n",
    "        \n",
    "## Latent Semantic Analysis\n",
    "\n",
    "- **Limitations to VSM:**\n",
    "    - Treats each word as distinct from all others\n",
    "        - Synonyms can be treated as different from one another (big & large)\n",
    "    - Treats all occurrences of a term as the same, regardless of context\n",
    "        - Polysemy: the same word has different meanings attached (taking a break vs. breaking something)\n",
    "    - Difficulty with very large documents\n",
    "        - The more words in a document the more opportunities to diverge from other documents in the space\n",
    "        - Can make finding similarities difficult\n",
    "- **Latent Semantic Analysis (LSA):** also known as Latent Semantic Indexing\n",
    "    - Apply PCA to a tf-idf term-doc matrix\n",
    "    - Reduces full matrix to a lower dimensional space\n",
    "    - Combines information from multiple terms into a single new one, resulting in fewer rows\n",
    "    - Outputs clusters of terms that (presumably) reflect a topic\n",
    "    - Each doc is scored for each topic, higher scores = higher relevance to the topic\n",
    "    - Docs can pertain to more than one topic\n",
    "- LSA is useful for:\n",
    "    - Very large corpus\n",
    "    - Unsure about which topics characterize documents\n",
    "    - Creating features to use in other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/brianmcguckin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/brianmcguckin/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "['[ Emma by Jane Austen 1816 ]', 'VOLUME I', 'CHAPTER I', 'Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her .']\n"
     ]
    }
   ],
   "source": [
    "#example: comparing paragrahs within Emma by Jane Austen\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#read data in paragraph form\n",
    "emma = gutenberg.paras('austen-emma.txt')\n",
    "\n",
    "emma_paras = []\n",
    "for paragraph in emma:\n",
    "    para = paragraph[0]\n",
    "    #remove double-dash\n",
    "    para = [re.sub(r'--','',word) for word in para]\n",
    "    #format each paragraph into a string, add to list of strings\n",
    "    emma_paras.append(' '.join(para))\n",
    "\n",
    "print(emma_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of feats: 1948\n",
      "original sentence: A very few minutes more , however , completed the present trial .\n",
      "tf_idf vector: {'minutes': 0.7127450310382584, 'present': 0.701423210857947}\n"
     ]
    }
   ],
   "source": [
    "#using sklearn tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(emma_paras, test_size=0.4, random_state=0)\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, #drop words occuring in more than half of the paragrahs\n",
    "                             min_df=2, #words must appear at least twice\n",
    "                             stop_words='english',\n",
    "                             lowercase=True, #lower case since alice capitalizes words for emphasis\n",
    "                             use_idf=True, #use idf for weighting\n",
    "                             norm=u'l2', #correction factor, longer & shorter paras treated equally\n",
    "                             smooth_idf=True) #adds one to all doc freqs, avoids div by 0 errors\n",
    "#apply vectorizer\n",
    "emma_paras_tfidf = vectorizer.fit_transform(emma_paras)\n",
    "print('no. of feats: %d' % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#train & test sets\n",
    "X_train_tfidf, X_test_tfidf = train_test_split(emma_paras_tfidf,\n",
    "                                               test_size=0.4,\n",
    "                                               random_state=0)\n",
    "#reshape output for readability\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#no. of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#list of dictionaries (one per para)\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#list of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#list feature words & tf-idf scores for each para\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i,j]\n",
    "    \n",
    "#since log base 2 of 1 = 0, tf-idf of 0 means word was present once\n",
    "print('original sentence:', X_train[5])\n",
    "print('tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent variance captured: 45.18983055539586\n",
      "component 0:\n",
      "\" Oh !     0.999284\n",
      "\" Oh !\"    0.999284\n",
      "\" Oh !     0.999284\n",
      "\" Oh !     0.999284\n",
      "\" Oh !\"    0.999284\n",
      "\" Oh !     0.999284\n",
      "\" Oh !     0.999284\n",
      "\" Oh !     0.999284\n",
      "\" Oh !     0.999284\n",
      "\" Oh !     0.999284\n",
      "Name: 0, dtype: float64\n",
      "component 1:\n",
      "\" You have made her too tall , Emma ,\" said Mr . Knightley .                                                                                                                0.634699\n",
      "\" You get upon delicate subjects , Emma ,\" said Mrs . Weston smiling ; \" remember that I am here . Mr .                                                                     0.576832\n",
      "\" I do not know what your opinion may be , Mrs . Weston ,\" said Mr . Knightley , \" of this great intimacy between Emma and Harriet Smith , but I think it a bad thing .\"    0.561547\n",
      "\" You are right , Mrs . Weston ,\" said Mr . Knightley warmly , \" Miss Fairfax is as capable as any of us of forming a just opinion of Mrs . Elton .                         0.560181\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .                                                                                                 0.528384\n",
      "Mr . Knightley might quarrel with her , but Emma could not quarrel with herself .                                                                                           0.525860\n",
      "Emma found that it was not Mr . Weston ' s fault that the number of privy councillors was not yet larger .                                                                  0.514557\n",
      "\" Now ,\" said Emma , when they were fairly beyond the sweep gates , \" now Mr . Weston , do let me know what has happened .\"                                                 0.510231\n",
      "\" In one respect , perhaps , Mr . Elton ' s manners are superior to Mr . Knightley ' s or Mr . Weston ' s .                                                                 0.505416\n",
      "\" I do not admire it ,\" said Mr . Knightley .                                                                                                                               0.499671\n",
      "Name: 1, dtype: float64\n",
      "component 2:\n",
      "CHAPTER X      0.998684\n",
      "CHAPTER V      0.998684\n",
      "CHAPTER I      0.998684\n",
      "CHAPTER V      0.998684\n",
      "CHAPTER V      0.998684\n",
      "CHAPTER X      0.998684\n",
      "CHAPTER X      0.998684\n",
      "CHAPTER I      0.998684\n",
      "CHAPTER I      0.998684\n",
      "CHAPTER XII    0.997664\n",
      "Name: 2, dtype: float64\n",
      "component 3:\n",
      "\" Ah !      0.992892\n",
      "\" Ah !      0.992892\n",
      "\" Ah !      0.992892\n",
      "\" Ah !      0.992892\n",
      "\" Ah !\"     0.992892\n",
      "\" Ah !      0.992892\n",
      "But ah !    0.992892\n",
      "\" Ah !      0.992892\n",
      "\" Ah !      0.992892\n",
      "\" Ah !      0.992892\n",
      "Name: 3, dtype: float64\n",
      "component 4:\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .    0.650630\n",
      "\" Are you well , my Emma ?\"                                                    0.598816\n",
      "Emma demurred .                                                                0.598816\n",
      "Emma was silenced .                                                            0.587753\n",
      "At first it was downright dulness to Emma .                                    0.586831\n",
      "\" Emma , my dear Emma \"                                                        0.577072\n",
      "Emma could not resist .                                                        0.566835\n",
      "\" It is not now worth a regret ,\" said Emma .                                  0.563831\n",
      "\" For shame , Emma !                                                           0.539777\n",
      "Emma was out of hearing .                                                      0.495813\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#dimension reduction: using SVD over PCA because PCA mean-centers variables (loses sparsity)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#SVD data reducer\n",
    "svd = TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "#run svd on training data then project training data\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained = svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print('percent variance captured: {}'.format(total_variance*100))\n",
    "\n",
    "#see what paragraphs are considered similar for first 5 topics\n",
    "paras_by_component = pd.DataFrame(X_train_lsa, index=X_train)\n",
    "for i in range(5):\n",
    "    print('component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:\n",
      "0 That is _court_ .\n",
      "1 \" Yes , sir , I did indeed ; and I am very much obliged by your kind solicitude about me .\"\n",
      "2 \" How much his business engrosses him already is very plain from the circumstance of his forgetting to inquire for the book you recommended .\n",
      "3 To restrain him as much as might be , by her own manners , she was immediately preparing to speak with exquisite calmness and gravity of the weather and the night ; but scarcely had she begun , scarcely had they passed the sweep - gate and joined the other carriage , than she found her subject cut up  her hand seized  her attention demanded , and Mr . Elton actually making violent love to her : availing himself of the precious opportunity , declaring sentiments which must be already well known , hoping  fearing  adoring  ready to die if she refused him ; but flattering himself that his ardent attachment and unequalled love and unexampled passion could not fail of having some effect , and in short , very much resolved on being seriously accepted as soon as possible .\n",
      "4 Emma smiled and answered \" My visit was of use to the nervous part of her complaint , I hope ; but not even I can charm away a sore throat ; it is a most severe cold indeed .\n",
      "5 A very few minutes more , however , completed the present trial .\n",
      "6 \" I am delighted to hear you speak so stoutly on the subject ,\" replied Emma , smiling ; \" but you do not mean to deny that there was a time  and not very distant either  when you gave me reason to understand that you did care about him ?\"\n",
      "7 \" Very well ; and if he had intended to give her one , he would have told her so .\"\n",
      "8 Some laughed , and answered good - humouredly .\n",
      "9 \" There appeared such a perfectly good understanding among them all \" he began rather quickly , but checking himself , added , \" however , it is impossible for me to say on what terms they really were  how it might all be behind the scenes .\n"
     ]
    }
   ],
   "source": [
    "#compute document similarity using lsa components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "\n",
    "#first 10 sentences\n",
    "sim_matrix = pd.DataFrame(similarity, index=X_train).iloc[0:10,0:10]\n",
    "\n",
    "#make plot\n",
    "ax = sns.heatmap(sim_matrix, yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#generate key for plot\n",
    "print('key:')\n",
    "for i in range(10):\n",
    "    print(i, sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 0: Test set\n",
    "Apply LSA model to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence: \" And I am quite serious too , I assure you ,\" replied Mrs . Elton gaily , \" in resolving to be always on the watch , and employing my friends to watch also , that nothing really unexceptionable may pass us .\"\n",
      "tf_idf vector: {'resolving': 0.2774191165193505, 'also': 0.23044247889104402, 'unexceptionable': 0.2589497988171934, 'assure': 0.21689513115302067, 'serious': 0.22172306773030476, 'friends': 0.20377398658436527, 'quite': 0.1552585713313441, 'replied': 0.16458059338939593, 'nothing': 0.16621195958420604, 'watch': 0.504715627998901, 'elton': 0.1367071745545188, 'may': 0.1679204262351428, 'pass': 0.23044247889104402, 'on': 0.12180212724011232, 'am': 0.1452704048380038, 'my': 0.12398388239402786, 'always': 0.17717390643975056, 'us': 0.19183382863312068, 'you': 0.09665948426551466, 'really': 0.17793510964706347, 'mrs': 0.11974456723163628, 'be': 0.097232748184252, 'that': 0.09665948426551466, 'too': 0.17033170735790376, 'in': 0.0946064191634381}\n"
     ]
    }
   ],
   "source": [
    "#reshape output for readability\n",
    "X_test_tfidf_csr = X_test_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_test_tfidf_csr.shape[0]\n",
    "\n",
    "#dictionaries\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#list of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#feature words and tf-idf scores\n",
    "for i, j in zip(*X_test_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_test_tfidf_csr[i,j]\n",
    "    \n",
    "print('original sentence:', X_test[5])\n",
    "print('tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent variance captured: 49.72676536945169\n"
     ]
    }
   ],
   "source": [
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "\n",
    "print('percent variance captured: {}'.format(\n",
    "    (svd.explained_variance_ratio_.sum()*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component 0:\n",
      "\" Oh !     0.780338\n",
      "\" Oh !     0.780338\n",
      "\" Oh !     0.780338\n",
      "\" Oh !     0.780338\n",
      "\" Oh !     0.780338\n",
      "\" Oh !     0.780338\n",
      "\" Oh !     0.780338\n",
      "\" Oh !     0.780338\n",
      "\" Oh !\"    0.780338\n",
      "\" Oh !     0.780338\n",
      "Name: 0, dtype: float64\n",
      "component 1:\n",
      "\" Oh !    0.624636\n",
      "\" Oh !    0.624636\n",
      "\" Oh !    0.624636\n",
      "\" Oh !    0.624636\n",
      "\" Oh !    0.624636\n",
      "\" Oh !    0.624636\n",
      "\" Oh !    0.624636\n",
      "\" Oh !    0.624636\n",
      "\" Oh !    0.624636\n",
      "\" Oh !    0.624636\n",
      "Name: 1, dtype: float64\n",
      "component 2:\n",
      "\" I thank you ; but I assure you you are quite mistaken .                                                                                         0.593853\n",
      "\" You are ungrateful .\"                                                                                                                           0.529507\n",
      "\" You are sick of prosperity and indulgence .                                                                                                     0.524702\n",
      "\" Emma , my love , you said that this circumstance would not now make you unhappy ; but I am afraid it gives you more pain than you expected .    0.508448\n",
      "\" My dear , you do not understand me .                                                                                                            0.507901\n",
      "\" When you have seen more of this country , I am afraid you will think you have overrated Hartfield .                                             0.486207\n",
      "\" Come ,\" said he , \" you are anxious for a compliment , so I will tell you that you have improved her .                                          0.474501\n",
      "\" Will you ?\"                                                                                                                                     0.473877\n",
      "\" You are right .                                                                                                                                 0.464788\n",
      "\" Ungrateful ! What do you mean ?\"                                                                                                                0.427369\n",
      "Name: 2, dtype: float64\n",
      "component 3:\n",
      "CHAPTER XVIII    0.998560\n",
      "CHAPTER XVIII    0.998560\n",
      "CHAPTER XV       0.998560\n",
      "CHAPTER XV       0.998560\n",
      "CHAPTER XVIII    0.998560\n",
      "CHAPTER XV       0.998560\n",
      "CHAPTER XIX      0.998560\n",
      "CHAPTER XII      0.997375\n",
      "CHAPTER XII      0.997375\n",
      "CHAPTER VI       0.997367\n",
      "Name: 3, dtype: float64\n",
      "component 4:\n",
      "\" Ah !     0.993021\n",
      "\" Ah !\"    0.993021\n",
      "\" Ah !     0.993021\n",
      "\" Ah !     0.993021\n",
      "\" Ah !     0.993021\n",
      "\" Ah !     0.993021\n",
      "\" Ah !     0.993021\n",
      "\" Ah !     0.993021\n",
      "\" Ah !     0.993021\n",
      "\" Ah !     0.993021\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "paras_by_component = pd.DataFrame(X_test_lsa, index=X_test)\n",
    "for i in range(5):\n",
    "    print('component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 1: tweaking tf-idf\n",
    "\n",
    "Change up decision points from original translation, see how LSA is affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set\n",
      "no. feats: 2174\n",
      "% variance captured: 49.72676536945169\n"
     ]
    }
   ],
   "source": [
    "#change max_df to a stricter threshold and leave stop words in\n",
    "vectorizer = TfidfVectorizer(max_df=0.25, #drop words occuring in more than 25% of the paragrahs\n",
    "                             min_df=2, #words must appear at least twice\n",
    "                             #stop_words='english',\n",
    "                             lowercase=True, #lower case since alice capitalizes words for emphasis\n",
    "                             use_idf=True, #use idf for weighting\n",
    "                             norm=u'l2', #correction factor, longer & shorter paras treated equally\n",
    "                             smooth_idf=True) #adds one to all doc freqs, avoids div by 0 errors\n",
    "#apply\n",
    "emma_paras_tfidf = vectorizer.fit_transform(emma_paras)\n",
    "print('train set')\n",
    "print('no. feats: %d' % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#train/test\n",
    "X_train_tfidf, X_test_tfidf = train_test_split(emma_paras_tfidf,\n",
    "                                               test_size=0.4,\n",
    "                                               random_state=0)\n",
    "\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#num paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#dicts\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#feats\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#words & scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i,j]\n",
    "    \n",
    "#dimension reduction\n",
    "svd = TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "print('% variance captured: {}'.format(\n",
    "    svd.explained_variance_ratio_.sum()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set\n",
      "no. feats: 997\n",
      "% variance captured: 57.92243127099559\n"
     ]
    }
   ],
   "source": [
    "#same as above plus increase min_df to 5\n",
    "vectorizer = TfidfVectorizer(max_df=0.25, #drop words occuring in more than 25% of the paragrahs\n",
    "                             min_df=5, #words must appear at least twice\n",
    "                             #stop_words='english',\n",
    "                             lowercase=True, #lower case since alice capitalizes words for emphasis\n",
    "                             use_idf=True, #use idf for weighting\n",
    "                             norm=u'l2', #correction factor, longer & shorter paras treated equally\n",
    "                             smooth_idf=True) #adds one to all doc freqs, avoids div by 0 errors\n",
    "#apply\n",
    "emma_paras_tfidf = vectorizer.fit_transform(emma_paras)\n",
    "print('train set')\n",
    "print('no. feats: %d' % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#train/test\n",
    "X_train_tfidf, X_test_tfidf = train_test_split(emma_paras_tfidf,\n",
    "                                               test_size=0.4,\n",
    "                                               random_state=0)\n",
    "\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#num paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#dicts\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#feats\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#words & scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i,j]\n",
    "    \n",
    "#dimension reduction\n",
    "svd = TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "print('% variance captured: {}'.format(\n",
    "    svd.explained_variance_ratio_.sum()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set\n",
      "no. feats: 799\n",
      "% variance captured: 55.8604452556428\n"
     ]
    }
   ],
   "source": [
    "#try same as above but put stop words back in\n",
    "vectorizer = TfidfVectorizer(max_df=0.25, #drop words occuring in more than 25% of the paragrahs\n",
    "                             min_df=5, #words must appear at least twice\n",
    "                             stop_words='english',\n",
    "                             lowercase=True, #lower case since alice capitalizes words for emphasis\n",
    "                             use_idf=True, #use idf for weighting\n",
    "                             norm=u'l2', #correction factor, longer & shorter paras treated equally\n",
    "                             smooth_idf=True) #adds one to all doc freqs, avoids div by 0 errors\n",
    "#apply\n",
    "emma_paras_tfidf = vectorizer.fit_transform(emma_paras)\n",
    "print('train set')\n",
    "print('no. feats: %d' % emma_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#train/test\n",
    "X_train_tfidf, X_test_tfidf = train_test_split(emma_paras_tfidf,\n",
    "                                               test_size=0.4,\n",
    "                                               random_state=0)\n",
    "\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#num paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "#dicts\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "#feats\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "#words & scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i,j]\n",
    "    \n",
    "#dimension reduction\n",
    "svd = TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "print('% variance captured: {}'.format(\n",
    "    svd.explained_variance_ratio_.sum()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
