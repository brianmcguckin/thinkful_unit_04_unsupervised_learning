{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Build your own NLP model\n",
    "\n",
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "1. Data cleaning / processing / language parsing\n",
    "2. Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "3. Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "4. Assess your models using cross-validation and determine whether one model performed better.\n",
    "5. Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "\n",
    "Write up your report in a Jupyter notebook. Be sure to explicitly justify the choices you make throughout, and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Morgan Freeman 'Devastated' That Sexual Harass...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/morgan-fr...</td>\n",
       "      <td>\"It is not right to equate horrific incidents ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Donald Trump Is Lovin' New McDonald's Jingle I...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donald-tr...</td>\n",
       "      <td>It's catchy, all right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Todd Van Luling</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>What To Watch On Amazon Prime That’s New This ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/amazon-pr...</td>\n",
       "      <td>There's a great mini-series joining this week.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Mike Myers Reveals He'd 'Like To' Do A Fourth ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mike-myer...</td>\n",
       "      <td>Myer's kids may be pushing for a new \"Powers\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Todd Van Luling</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>What To Watch On Hulu That’s New This Week</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hulu-what...</td>\n",
       "      <td>You're getting a recent Academy Award-winning ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "5       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "6       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "7  Todd Van Luling  ENTERTAINMENT 2018-05-26   \n",
       "8    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "9  Todd Van Luling  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "5  Morgan Freeman 'Devastated' That Sexual Harass...   \n",
       "6  Donald Trump Is Lovin' New McDonald's Jingle I...   \n",
       "7  What To Watch On Amazon Prime That’s New This ...   \n",
       "8  Mike Myers Reveals He'd 'Like To' Do A Fourth ...   \n",
       "9         What To Watch On Hulu That’s New This Week   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "5  https://www.huffingtonpost.com/entry/morgan-fr...   \n",
       "6  https://www.huffingtonpost.com/entry/donald-tr...   \n",
       "7  https://www.huffingtonpost.com/entry/amazon-pr...   \n",
       "8  https://www.huffingtonpost.com/entry/mike-myer...   \n",
       "9  https://www.huffingtonpost.com/entry/hulu-what...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  \n",
       "5  \"It is not right to equate horrific incidents ...  \n",
       "6                            It's catchy, all right.  \n",
       "7     There's a great mini-series joining this week.  \n",
       "8  Myer's kids may be pushing for a new \"Powers\" ...  \n",
       "9  You're getting a recent Academy Award-winning ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_raw = pd.read_json('News_Category_Dataset.json', lines=True)\n",
    "news_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate business and sports\n",
    "business_df = news_raw.loc[news_raw['category']=='BUSINESS']\n",
    "sports_df = news_raw.loc[news_raw['category']=='SPORTS']\n",
    "\n",
    "#get text data\n",
    "business_head = business_df['headline'].tolist()\n",
    "business_desc = business_df['short_description'].tolist()\n",
    "business_raw = [a + ' ' + b for a, b in zip(business_head, business_desc)]\n",
    "\n",
    "sports_head = sports_df['headline'].tolist()\n",
    "sports_desc = sports_df['short_description'].tolist()\n",
    "sports_raw = [a + ' ' + b for a, b in zip(sports_head, sports_desc)]\n",
    "\n",
    "#make docs from lists of strings\n",
    "business = ' '.join(business_raw)\n",
    "sports = ' '.join(sports_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text for spacy\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text) #replace -- with blank string\n",
    "    text = re.sub('[\\[].*?[\\]]','',text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "business_clean = text_cleaner(business)\n",
    "sports_clean = text_cleaner(sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jets Chairman Christopher Johnson Won't Fine Players For Anthem Protests “I never want to put restri\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_clean[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse cleaned text\n",
    "nlp = spacy.load('en')\n",
    "business_doc = nlp(business_clean)\n",
    "sports_doc = nlp(sports_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146683\n",
      "105483\n"
     ]
    }
   ],
   "source": [
    "print(len(business_doc))\n",
    "print(len(sports_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10731\n",
      "10462\n"
     ]
    }
   ],
   "source": [
    "#group into sentences\n",
    "business_sents = [[sent, 'business'] for sent in business_doc.sents]\n",
    "sports_sents = [[sent, 'sports'] for sent in sports_doc.sents]\n",
    "\n",
    "print(len(business_sents))\n",
    "print(len(sports_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_sample = business_sents[:1500]\n",
    "sports_sample = sports_sents[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine sentences into single df\n",
    "sentences = pd.DataFrame(bus_sample + sports_sample)\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(U.S., Launches, Auto, Import, Probe, ,, China...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(To, Defend, Its, Interests)</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(The, investigation, could, lead, to, new, U.S...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Starbucks, Says, Anyone, Can, Now, Sit, In, I...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Even, Without, Buying, Anything, The, new, po...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0         1\n",
       "0  (U.S., Launches, Auto, Import, Probe, ,, China...  business\n",
       "1                       (To, Defend, Its, Interests)  business\n",
       "2  (The, investigation, could, lead, to, new, U.S...  business\n",
       "3  (Starbucks, Says, Anyone, Can, Now, Sit, In, I...  business\n",
       "4  (Even, Without, Buying, Anything, The, new, po...  business"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW, exclude stopwords & punctuation, use lemmas, 2000 most common words\n",
    "from collections import Counter\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    #filter punct and stopwords\n",
    "    allwords = [token.lemma_ for token in text \n",
    "                if not token.is_punct and not token.is_stop]\n",
    "    \n",
    "    #return most common words\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "#creates a dataframe with features for each word in common word set\n",
    "#values are count of times word appears in each sentence\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    #set df and initialize counts\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    #process each row, counting word occurences\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        #convert sentence to lemmas & filter punct, stops, & uncommon words\n",
    "        words = [token.lemma_ for token in sentence\n",
    "                 if (not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words)]\n",
    "        \n",
    "        #populate row with word counts\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        #counter to make sure kernal isn't hanging\n",
    "        if i % 250 == 0:\n",
    "            print('processing row {}'.format(i))\n",
    "            print(time.clock())\n",
    "    \n",
    "    return df\n",
    "\n",
    "#set up bags\n",
    "businesswords = bag_of_words(business_doc)\n",
    "sportswords = bag_of_words(sports_doc)\n",
    "\n",
    "#combine bags to create set of unique words\n",
    "common_words = set(businesswords + sportswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2959)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_counts = bow_features(sentences, common_words)\n",
    "#word_counts.to_csv('bow_features_business_sports', index=False)\n",
    "word_counts = pd.read_csv('bow_features_business_sports')\n",
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9683333333333334\n",
      "test score: 0.7416666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(word_counts.drop(['text_sentence', 'text_source'], 1))\n",
    "y = word_counts['text_source']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier().fit(X_train, y_train)\n",
    "print('train score: {}'.format(rfc.score(X_train, y_train)))\n",
    "print('test score: {}'.format(rfc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9405555555555556\n",
      "test score: 0.7908333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print('train score: {}'.format(lr.score(X_train, y_train)))\n",
    "print('test score: {}'.format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.7838888888888889\n",
      "test score: 0.7333333333333333\n",
      "14.314459 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "gbc = ensemble.GradientBoostingClassifier().fit(X_train, y_train)\n",
    "print('train score: {}'.format(gbc.score(X_train, y_train)))\n",
    "print('test score: {}'.format(gbc.score(X_test, y_test)))\n",
    "print('{} seconds'.format(time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.7838888888888889\n",
      "test score: 0.7333333333333333\n",
      "7.013244999999998 seconds\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "start_time = time.clock()\n",
    "xgbc = xgb.XGBClassifier().fit(X_train, y_train)\n",
    "print('train score: {}'.format(gbc.score(X_train, y_train)))\n",
    "print('test score: {}'.format(gbc.score(X_test, y_test)))\n",
    "print('{} seconds'.format(time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsample of data was pretty small given the original size\n",
    "# try a larger sample and see if models improve\n",
    "bus_sample_3000 = business_sents[:3000]\n",
    "sports_sample_3000 = sports_sents[:3000]\n",
    "sentences = pd.DataFrame(bus_sample_3000 + sports_sample_3000)\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing row 0\n",
      "113.660745\n",
      "processing row 50\n",
      "369.840926\n",
      "processing row 100\n",
      "602.289914\n",
      "processing row 150\n",
      "813.694071\n",
      "processing row 200\n",
      "1055.261497\n",
      "processing row 250\n",
      "1279.636973\n",
      "processing row 300\n",
      "1481.472321\n",
      "processing row 350\n",
      "1717.36095\n",
      "processing row 400\n",
      "1965.289835\n",
      "processing row 450\n",
      "2210.596457\n",
      "processing row 500\n",
      "2453.024662\n",
      "processing row 550\n",
      "2679.848909\n",
      "processing row 600\n",
      "2939.132854\n",
      "processing row 650\n",
      "3161.681253\n",
      "processing row 700\n",
      "3406.50285\n",
      "processing row 750\n",
      "3605.608094\n",
      "processing row 800\n",
      "3840.61901\n",
      "processing row 850\n",
      "4093.627801\n",
      "processing row 900\n",
      "4386.460868\n",
      "processing row 950\n",
      "4638.441826\n",
      "processing row 1000\n",
      "4903.51757\n",
      "processing row 1050\n",
      "5174.622038\n",
      "processing row 1100\n",
      "5388.965507\n",
      "processing row 1150\n",
      "5576.217431\n",
      "processing row 1200\n",
      "5837.319115\n",
      "processing row 1250\n",
      "6084.650505\n",
      "processing row 1300\n",
      "6311.531316\n",
      "processing row 1350\n",
      "6534.507609\n",
      "processing row 1400\n",
      "6777.303552\n",
      "processing row 1450\n",
      "6971.18641\n",
      "processing row 1500\n",
      "7201.50416\n",
      "processing row 1550\n",
      "7409.436992\n",
      "processing row 1600\n",
      "7657.128203\n",
      "processing row 1650\n",
      "7908.427838\n",
      "processing row 1700\n",
      "8093.485879\n",
      "processing row 1750\n",
      "8323.129155\n",
      "processing row 1800\n",
      "8546.647153\n",
      "processing row 1850\n",
      "8798.996313\n",
      "processing row 1900\n",
      "9039.584153\n",
      "processing row 1950\n",
      "9266.066698\n",
      "processing row 2000\n",
      "9444.963215\n",
      "processing row 2050\n",
      "9650.095389\n",
      "processing row 2100\n",
      "9885.278696\n",
      "processing row 2150\n",
      "10112.032699\n",
      "processing row 2200\n",
      "10349.530519\n",
      "processing row 2250\n",
      "10553.694944\n",
      "processing row 2300\n",
      "10784.724958\n",
      "processing row 2350\n",
      "10975.489015\n",
      "processing row 2400\n",
      "11206.661521\n",
      "processing row 2450\n",
      "11471.383535\n",
      "processing row 2500\n",
      "11744.780456\n",
      "processing row 2550\n",
      "11960.193965\n",
      "processing row 2600\n",
      "12258.917789\n",
      "processing row 2650\n",
      "12447.453421\n",
      "processing row 2700\n",
      "12697.975908\n",
      "processing row 2750\n",
      "12926.020889\n",
      "processing row 2800\n",
      "13186.630869\n",
      "processing row 2850\n",
      "13434.088379\n",
      "processing row 2900\n",
      "13688.08792\n",
      "processing row 2950\n",
      "13973.644841\n",
      "processing row 3000\n",
      "14213.89087\n",
      "processing row 3050\n",
      "14440.212998\n",
      "processing row 3100\n",
      "14687.818917\n",
      "processing row 3150\n",
      "14933.123657\n",
      "processing row 3200\n",
      "15133.461946\n",
      "processing row 3250\n",
      "15375.068645\n",
      "processing row 3300\n",
      "15566.617861\n",
      "processing row 3350\n",
      "15782.51517\n",
      "processing row 3400\n",
      "15972.374012\n",
      "processing row 3450\n",
      "16176.415664\n",
      "processing row 3500\n",
      "16374.437529\n",
      "processing row 3550\n",
      "16568.609867\n",
      "processing row 3600\n",
      "16752.52\n",
      "processing row 3650\n",
      "16950.445146\n",
      "processing row 3700\n",
      "17155.811062\n",
      "processing row 3750\n",
      "17371.955161\n",
      "processing row 3800\n",
      "17549.229691\n",
      "processing row 3850\n",
      "17793.572663\n",
      "processing row 3900\n",
      "17998.89904\n",
      "processing row 3950\n",
      "18214.755962\n",
      "processing row 4000\n",
      "18403.007856\n",
      "processing row 4050\n",
      "18600.831352\n",
      "processing row 4100\n",
      "18806.778146\n",
      "processing row 4150\n",
      "19042.959935\n",
      "processing row 4200\n",
      "19262.07563\n",
      "processing row 4250\n",
      "19463.775054\n",
      "processing row 4300\n",
      "19674.521544\n",
      "processing row 4350\n",
      "19935.242813\n",
      "processing row 4400\n",
      "20160.222049\n",
      "processing row 4450\n",
      "20446.811123\n",
      "processing row 4500\n",
      "20637.240795\n",
      "processing row 4550\n",
      "20834.957254\n",
      "processing row 4600\n",
      "21030.272214\n",
      "processing row 4650\n",
      "21232.005446\n",
      "processing row 4700\n",
      "21450.432076\n",
      "processing row 4750\n",
      "21640.18024\n",
      "processing row 4800\n",
      "21874.52977\n",
      "processing row 4850\n",
      "22101.633786\n",
      "processing row 4900\n",
      "22352.474591\n",
      "processing row 4950\n",
      "22567.285643\n",
      "processing row 5000\n",
      "22785.67999\n",
      "processing row 5050\n",
      "23005.199032\n",
      "processing row 5100\n",
      "23198.314619\n",
      "processing row 5150\n",
      "23441.97468\n",
      "processing row 5200\n",
      "23693.444851\n",
      "processing row 5250\n",
      "23900.270531\n",
      "processing row 5300\n",
      "24096.840129\n",
      "processing row 5350\n",
      "24280.918114\n",
      "processing row 5400\n",
      "24484.591381\n",
      "processing row 5450\n",
      "24728.879219\n",
      "processing row 5500\n",
      "24948.689485\n",
      "processing row 5550\n",
      "25209.018342\n",
      "processing row 5600\n",
      "25445.359595\n",
      "processing row 5650\n",
      "25651.363234\n",
      "processing row 5700\n",
      "25891.522858\n",
      "processing row 5750\n",
      "26137.135456\n",
      "processing row 5800\n",
      "26396.640543\n",
      "processing row 5850\n",
      "26621.197732\n",
      "processing row 5900\n",
      "26840.851547\n",
      "processing row 5950\n",
      "27057.796652\n"
     ]
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.to_csv('bow_features_6000_business_sports', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9713888888888889\n",
      "test score: 0.7479166666666667\n"
     ]
    }
   ],
   "source": [
    "X = np.array(word_counts.drop(['text_sentence', 'text_source'], 1))\n",
    "y = word_counts['text_source']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier().fit(X_train, y_train)\n",
    "print('train score: {}'.format(rfc.score(X_train, y_train)))\n",
    "print('test score: {}'.format(rfc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9277777777777778\n",
      "test score: 0.8091666666666667\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print('train score: {}'.format(lr.score(X_train, y_train)))\n",
    "print('test score: {}'.format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.7452777777777778\n",
      "test score: 0.69625\n",
      "46.18460899999991 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "gbc = ensemble.GradientBoostingClassifier().fit(X_train, y_train)\n",
    "print('train score: {}'.format(gbc.score(X_train, y_train)))\n",
    "print('test score: {}'.format(gbc.score(X_test, y_test)))\n",
    "print('{} seconds'.format(time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.7452777777777778\n",
      "test score: 0.69625\n",
      "15.65634399999908 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "xgbc = xgb.XGBClassifier().fit(X_train, y_train)\n",
    "print('train score: {}'.format(gbc.score(X_train, y_train)))\n",
    "print('test score: {}'.format(gbc.score(X_test, y_test)))\n",
    "print('{} seconds'.format(time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_bus = []\n",
    "for sentence in business_doc.sents:\n",
    "    sentence = [token.lemma_.lower()\n",
    "                for token in sentence\n",
    "                if not token.is_stop\n",
    "                and not token.is_punct]\n",
    "    sentences_bus.append(sentence)\n",
    "    \n",
    "sentences_sports = []\n",
    "for sentence in sports_doc.sents:\n",
    "    sentence = [token.lemma_.lower()\n",
    "                for token in sentence\n",
    "                if not token.is_stop\n",
    "                and not token.is_punct]\n",
    "    sentences_sports.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 1.373656000000011 seconds\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "start_time = time.clock()\n",
    "w2v = word2vec.Word2Vec(sentences_bus,\n",
    "                        workers=2,\n",
    "                        min_count=10,\n",
    "                        window=6,\n",
    "                        sg=0,\n",
    "                        sample=1e-3,\n",
    "                        size=300,\n",
    "                        hs=1)\n",
    "print('runtime: {} seconds'.format(time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = w2v.wv.vocab.keys()\n",
    "\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gop', 0.9502933025360107), ('buffett', 0.9450339078903198), ('december', 0.9429740905761719), ('world', 0.9420689940452576), ('responsibility', 0.9412590265274048), ('vote', 0.9407662749290466), ('seattle', 0.9399857521057129), ('global', 0.9397921562194824), ('elizabeth', 0.9389846920967102), ('name', 0.9381713271141052)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv.most_similar(positive=['investigation', 'trump'], negative=['facebook']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9844992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv.similarity('investigation', 'probe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 0.9873839999999916 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "w2v = word2vec.Word2Vec(sentences_sports,\n",
    "                        workers=2,\n",
    "                        min_count=10,\n",
    "                        window=6,\n",
    "                        sg=0,\n",
    "                        sample=1e-3,\n",
    "                        size=300,\n",
    "                        hs=1)\n",
    "print('runtime: {} seconds'.format(time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = w2v.wv.vocab.keys()\n",
    "\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('really', 0.9863007068634033), ('think', 0.9798712134361267), ('friend', 0.9792352318763733), ('nothing', 0.976024866104126), ('penn', 0.9752417206764221), ('witness', 0.9738208055496216), ('draft', 0.9735362529754639), ('grand', 0.9729882478713989), ('tweet', 0.9694628715515137), ('idea', 0.9687024354934692)]\n",
      "0.93453765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv.most_similar(positive=['anthem', 'protests'], negative=['kaepernick']))\n",
    "print(w2v.similarity('anthem', 'protests'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
