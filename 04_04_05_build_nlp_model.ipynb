{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Build your own NLP model\n",
    "\n",
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "1. Data cleaning / processing / language parsing\n",
    "2. Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "3. Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "4. Assess your models using cross-validation and determine whether one model performed better.\n",
    "5. Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "\n",
    "Write up your report in a Jupyter notebook. Be sure to explicitly justify the choices you make throughout, and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Morgan Freeman 'Devastated' That Sexual Harass...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/morgan-fr...</td>\n",
       "      <td>\"It is not right to equate horrific incidents ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Donald Trump Is Lovin' New McDonald's Jingle I...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donald-tr...</td>\n",
       "      <td>It's catchy, all right.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Todd Van Luling</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>What To Watch On Amazon Prime That’s New This ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/amazon-pr...</td>\n",
       "      <td>There's a great mini-series joining this week.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Mike Myers Reveals He'd 'Like To' Do A Fourth ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mike-myer...</td>\n",
       "      <td>Myer's kids may be pushing for a new \"Powers\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Todd Van Luling</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>What To Watch On Hulu That’s New This Week</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hulu-what...</td>\n",
       "      <td>You're getting a recent Academy Award-winning ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "5       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "6       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "7  Todd Van Luling  ENTERTAINMENT 2018-05-26   \n",
       "8    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "9  Todd Van Luling  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "5  Morgan Freeman 'Devastated' That Sexual Harass...   \n",
       "6  Donald Trump Is Lovin' New McDonald's Jingle I...   \n",
       "7  What To Watch On Amazon Prime That’s New This ...   \n",
       "8  Mike Myers Reveals He'd 'Like To' Do A Fourth ...   \n",
       "9         What To Watch On Hulu That’s New This Week   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "5  https://www.huffingtonpost.com/entry/morgan-fr...   \n",
       "6  https://www.huffingtonpost.com/entry/donald-tr...   \n",
       "7  https://www.huffingtonpost.com/entry/amazon-pr...   \n",
       "8  https://www.huffingtonpost.com/entry/mike-myer...   \n",
       "9  https://www.huffingtonpost.com/entry/hulu-what...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  \n",
       "5  \"It is not right to equate horrific incidents ...  \n",
       "6                            It's catchy, all right.  \n",
       "7     There's a great mini-series joining this week.  \n",
       "8  Myer's kids may be pushing for a new \"Powers\" ...  \n",
       "9  You're getting a recent Academy Award-winning ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_raw = pd.read_json('News_Category_Dataset.json', lines=True)\n",
    "news_raw.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate business and sports\n",
    "business_df = news_raw.loc[news_raw['category']=='BUSINESS']\n",
    "sports_df = news_raw.loc[news_raw['category']=='SPORTS']\n",
    "\n",
    "#get text data\n",
    "business_head = business_df['headline'].tolist()\n",
    "business_desc = business_df['short_description'].tolist()\n",
    "business_raw = [a + ' ' + b for a, b in zip(business_head, business_desc)]\n",
    "\n",
    "sports_head = sports_df['headline'].tolist()\n",
    "sports_desc = sports_df['short_description'].tolist()\n",
    "sports_raw = [a + ' ' + b for a, b in zip(sports_head, sports_desc)]\n",
    "\n",
    "#make docs from lists of strings\n",
    "business = ' '.join(business_raw)\n",
    "sports = ' '.join(sports_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text for spacy\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text) #replace -- with blank string\n",
    "    text = re.sub('[\\[].*?[\\]]','',text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "business_clean = text_cleaner(business)\n",
    "sports_clean = text_cleaner(sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jets Chairman Christopher Johnson Won't Fine Players For Anthem Protests “I never want to put restri\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_clean[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse cleaned text\n",
    "nlp = spacy.load('en')\n",
    "business_doc = nlp(business_clean)\n",
    "sports_doc = nlp(sports_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146683\n",
      "105483\n"
     ]
    }
   ],
   "source": [
    "print(len(business_doc))\n",
    "print(len(sports_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10731\n",
      "10462\n"
     ]
    }
   ],
   "source": [
    "#group into sentences\n",
    "business_sents = [[sent, 'business'] for sent in business_doc.sents]\n",
    "sports_sents = [[sent, 'sports'] for sent in sports_doc.sents]\n",
    "\n",
    "print(len(business_sents))\n",
    "print(len(sports_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_sample = business_sents[:1500]\n",
    "sports_sample = sports_sents[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine sentences into single df\n",
    "sentences = pd.DataFrame(bus_sample + sports_sample)\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(U.S., Launches, Auto, Import, Probe, ,, China...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(To, Defend, Its, Interests)</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(The, investigation, could, lead, to, new, U.S...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Starbucks, Says, Anyone, Can, Now, Sit, In, I...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Even, Without, Buying, Anything, The, new, po...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0         1\n",
       "0  (U.S., Launches, Auto, Import, Probe, ,, China...  business\n",
       "1                       (To, Defend, Its, Interests)  business\n",
       "2  (The, investigation, could, lead, to, new, U.S...  business\n",
       "3  (Starbucks, Says, Anyone, Can, Now, Sit, In, I...  business\n",
       "4  (Even, Without, Buying, Anything, The, new, po...  business"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW, exclude stopwords & punctuation, use lemmas, 2000 most common words\n",
    "from collections import Counter\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    #filter punct and stopwords\n",
    "    allwords = [token.lemma_ for token in text \n",
    "                if not token.is_punct and not token.is_stop]\n",
    "    \n",
    "    #return most common words\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "\n",
    "#creates a dataframe with features for each word in common word set\n",
    "#values are count of times word appears in each sentence\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    #set df and initialize counts\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    #process each row, counting word occurences\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        #convert sentence to lemmas & filter punct, stops, & uncommon words\n",
    "        words = [token.lemma_ for token in sentence\n",
    "                 if (not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words)]\n",
    "        \n",
    "        #populate row with word counts\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        #counter to make sure kernal isn't hanging\n",
    "        if i % 50 == 0:\n",
    "            print('processing row {}'.format(i))\n",
    "            print(time.clock())\n",
    "    \n",
    "    return df\n",
    "\n",
    "#set up bags\n",
    "businesswords = bag_of_words(business_doc)\n",
    "sportswords = bag_of_words(sports_doc)\n",
    "\n",
    "#combine bags to create set of unique words\n",
    "common_words = set(businesswords + sportswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 2959)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_counts = bow_features(sentences, common_words)\n",
    "#word_counts.to_csv('bow_features_business_sports', index=False)\n",
    "word_counts = pd.read_csv('bow_features_business_sports')\n",
    "word_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9727777777777777\n",
      "test score: 0.7558333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(word_counts.drop(['text_sentence', 'text_source'], 1))\n",
    "y = word_counts['text_source']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier().fit(X_train, y_train)\n",
    "print('train score: {}'.format(rfc.score(X_train, y_train)))\n",
    "print('test score: {}'.format(rfc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9416666666666667\n",
      "test score: 0.7925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "print('train score: {}'.format(lr.score(X_train, y_train)))\n",
    "print('test score: {}'.format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.7733333333333333\n",
      "test score: 0.7275\n",
      "13.656545999999992 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "gbc = ensemble.GradientBoostingClassifier().fit(X_train, y_train)\n",
    "print('train score: {}'.format(gbc.score(X_train, y_train)))\n",
    "print('test score: {}'.format(gbc.score(X_test, y_test)))\n",
    "print('{} seconds'.format(time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.7733333333333333\n",
      "test score: 0.7275\n",
      "6.832531000000003 seconds\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "start_time = time.clock()\n",
    "xgbc = xgb.XGBClassifier().fit(X_train, y_train)\n",
    "print('train score: {}'.format(gbc.score(X_train, y_train)))\n",
    "print('test score: {}'.format(gbc.score(X_test, y_test)))\n",
    "print('{} seconds'.format(time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_bus = []\n",
    "for sentence in business_doc.sents:\n",
    "    sentence = [token.lemma_.lower()\n",
    "                for token in sentence\n",
    "                if not token.is_stop\n",
    "                and not token.is_punct]\n",
    "    sentences_bus.append(sentence)\n",
    "    \n",
    "sentences_sports = []\n",
    "for sentence in sports_doc.sents:\n",
    "    sentence = [token.lemma_.lower()\n",
    "                for token in sentence\n",
    "                if not token.is_stop\n",
    "                and not token.is_punct]\n",
    "    sentences_sports.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 1.2770589999999942 seconds\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "start_time = time.clock()\n",
    "w2v = word2vec.Word2Vec(sentences_bus,\n",
    "                        workers=2,\n",
    "                        min_count=10,\n",
    "                        window=6,\n",
    "                        sg=0,\n",
    "                        sample=1e-3,\n",
    "                        size=300,\n",
    "                        hs=1)\n",
    "print('runtime: {} seconds'.format(time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = w2v.wv.vocab.keys()\n",
    "\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gop', 0.9511142373085022), ('world', 0.9474011063575745), ('vote', 0.9472444653511047), ('december', 0.945892333984375), ('global', 0.9456270933151245), ('name', 0.9448801875114441), ('responsibility', 0.9428839087486267), ('elizabeth', 0.9415732622146606), ('buffett', 0.9403877258300781), ('seattle', 0.9400784969329834)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv.most_similar(positive=['investigation', 'trump'], negative=['facebook']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9846101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv.similarity('investigation', 'probe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 1.1487149999999957 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "w2v = word2vec.Word2Vec(sentences_sports,\n",
    "                        workers=2,\n",
    "                        min_count=10,\n",
    "                        window=6,\n",
    "                        sg=0,\n",
    "                        sample=1e-3,\n",
    "                        size=300,\n",
    "                        hs=1)\n",
    "print('runtime: {} seconds'.format(time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = w2v.wv.vocab.keys()\n",
    "\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('really', 0.9814202785491943), ('think', 0.9782571196556091), ('friend', 0.9777220487594604), ('nothing', 0.9771713614463806), ('penn', 0.9717471599578857), ('grand', 0.9705144166946411), ('draft', 0.9703025221824646), ('idea', 0.9695332050323486), ('witness', 0.9674822092056274), ('tweet', 0.9669411182403564)]\n",
      "0.931003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv.most_similar(positive=['anthem', 'protests'], negative=['kaepernick']))\n",
    "print(w2v.similarity('anthem', 'protests'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
