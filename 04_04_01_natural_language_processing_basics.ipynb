{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to NLP\n",
    "- So far data has been at least minimally processed, i.e. translated into numerical values and organized into variables\n",
    "- Most available information is verbal; words are full of data\n",
    "\n",
    "## Processing and analysis\n",
    "- NLP as a two-part problem:\n",
    "    1. Process data from its original form (text or speech) into one a computer can understand\n",
    "    2. Conduct analysis on the processed data\n",
    "- Step 1 involves cleaning and/or feature extraction\n",
    "    - **Language parsing:** dealing with verbal information\n",
    "    - Domain knowledge: word frequency, meaning, grammar, used to extract features of interest\n",
    "    - Already did some light language parsing building naive bayes spam filter\n",
    "    \n",
    "### NLP Packages\n",
    "- **NLTK (Natural Language ToolKit):**\n",
    "    - Customizable and transparent (good for learning)\n",
    "    - Contains older models/methods that may not be optimal for production code\n",
    "- **spaCy:** \n",
    "    - Processes text using latest & greatest algorithms/methods\n",
    "    - Leaner & faster than NLTK\n",
    "    - Loose choice, if spaCy algos change, results may change\n",
    "    - Written in Cython (python translated into C then run)\n",
    "- **re:** regular expressions library to pull out specific elements from strings (then passed onto spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "\n",
      "Raw:\n",
      " [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "#grab and process raw data\n",
    "print(gutenberg.fileids())\n",
    "\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "#print first 100 chars of alice in wonderland\n",
    "print('\\nRaw:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title removed:\n",
      " \n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on\n"
     ]
    }
   ],
   "source": [
    "#remove title, match text and replace with empty string\n",
    "pattern = '[\\[].*?[\\]]'\n",
    "persuasion = re.sub(pattern,'', persuasion)\n",
    "alice = re.sub(pattern, '', alice)\n",
    "\n",
    "print('title removed:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter headings removed:\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothin\n"
     ]
    }
   ],
   "source": [
    "#match & remove chapter headings\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "print('chapter headings removed:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra whitespace removed:\n",
      " Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to\n"
     ]
    }
   ],
   "source": [
    "#remove newlines and other whitespace by splitting & rejoining\n",
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "\n",
    "print('extra whitespace removed:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What information can we extract from text?\n",
    "\n",
    "### Tokens\n",
    "- **Token:** individual meaningful piece from a text, generally words & punctuation\n",
    "- **Tokenization:** process of breaking up text into tokens\n",
    "- May discard some tokens that don't add informational value (such as punctuation)\n",
    "- Stop words:\n",
    "    - Class of potentially uninformative tokens\n",
    "    - Includes frequently used words without much informational value ('the', 'of', etc)\n",
    "    - May or may not be discarded based on NLP approach    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#stopwords identified by NLTK\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#parse novels into tokens using spacy\n",
    "#calling spacy on the novel immediately & automatically parses it\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alice_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 34430 tokens long\n",
      "The first three tokens are 'Alice was beginning'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "#explore objects\n",
    "print(\"The alice_doc object is a {} object.\".format(type(alice_doc)))\n",
    "print(\"It is {} tokens long\".format(len(alice_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(alice_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(alice_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice: [('the', 1524), ('and', 796), ('to', 724), ('a', 611), ('I', 534), ('it', 524), ('she', 508), ('of', 499), ('said', 453), ('Alice', 394)]\n",
      "persuasion: [('the', 3120), ('to', 2775), ('and', 2738), ('of', 2563), ('a', 1529), ('in', 1346), ('was', 1329), ('had', 1177), ('her', 1159), ('I', 1121)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#extract information from tokenized text data\n",
    "#count how often various tokens occur\n",
    "\n",
    "#utility function to calculate how frequently words appear in text\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    #build list of words, strip punctuation and give option for stop words\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "    \n",
    "    #build and return counter object containing word counts\n",
    "    return Counter(words)\n",
    "\n",
    "#get most frequent words\n",
    "alice_freq = word_frequencies(alice_doc).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc).most_common(10)\n",
    "print('alice:', alice_freq)\n",
    "print('persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice: [('I', 534), ('said', 453), ('Alice', 394), (\"n't\", 215), (\"'s\", 190), ('little', 124), ('The', 102), ('like', 84), ('went', 83), ('know', 83)]\n",
      "persuasion: [('I', 1121), ('Anne', 497), (\"'s\", 485), ('She', 326), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('Mr', 255), ('He', 225), ('Wentworth', 217)]\n"
     ]
    }
   ],
   "source": [
    "#run again removing stop words using optional keyword argument\n",
    "alice_freq = word_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "print('alice:', alice_freq)\n",
    "print('persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique to alice: {'went', 'know', \"n't\", 'Alice', 'said', 'like', 'little', 'The'}\n",
      "unique to persuasion: {'Mr', 'Wentworth', 'Mrs', 'Anne', 'He', 'She', 'Captain', 'Elliot'}\n"
     ]
    }
   ],
   "source": [
    "#remove words in top 10 for both books\n",
    "alice_common = [pair[0] for pair in alice_freq]\n",
    "persuasion_common = [pair[0] for pair in persuasion_freq]\n",
    "\n",
    "#use sets to find unique values in each top 10\n",
    "print('unique to alice:', set(alice_common) - set(persuasion_common))\n",
    "print('unique to persuasion:', set(persuasion_common) - set(alice_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas\n",
    "- Use lemma (root word) to focus on an action or concept without splitting across all different forms of a word\n",
    "    - I.e. think, thought, thinking\n",
    "- Build a count of concepts by reducing words to their lemma and do counts again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice: [('-PRON-', 758), ('say', 476), ('alice', 396), ('be', 254), ('not', 231), ('go', 133), ('think', 131), ('little', 126), ('the', 109), ('look', 105)]\n",
      "persuasion: [('-PRON-', 2241), ('anne', 497), (\"'s\", 466), ('captain', 303), ('elliot', 295), ('mrs', 291), ('good', 289), ('know', 258), ('think', 256), ('mr', 255)]\n",
      "Unique to Alice: {'look', 'be', 'the', 'say', 'not', 'little', 'alice', 'go'}\n",
      "Unique to Persuasion: {\"'s\", 'know', 'mrs', 'elliot', 'good', 'anne', 'mr', 'captain'}\n"
     ]
    }
   ],
   "source": [
    "#function to calculate lemma frequency\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    #build list of lemmas, strip punctuation and give option for stopwords\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    #counter object\n",
    "    return Counter(lemmas)\n",
    "\n",
    "#instatiate lists of common lemmas\n",
    "alice_lemma_freq = lemma_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "print('\\nAlice:', alice_lemma_freq)\n",
    "print('persuasion:', persuasion_lemma_freq)\n",
    "\n",
    "#id the lemmas common to one text but not the other\n",
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common))\n",
    "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences\n",
    "\n",
    "- Split text into sentences using punctuation\n",
    "- Sentiment analysis can categorize each sentence as positive or negative\n",
    "- Sentence length, unique words, and contextual information can also be useful\n",
    "- Use spaCy doc.sents to get each sentence as a span object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice has 1678 sentences.\n",
      "example sentence from alice: \n",
      "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n"
     ]
    }
   ],
   "source": [
    "sentences = list(alice_doc.sents)\n",
    "print('alice has {} sentences.'.format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print('example sentence from alice: \\n{}'.format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 29 words and 25 are unique\n"
     ]
    }
   ],
   "source": [
    "#some metrics for this sentence\n",
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print(('there are {} words and {} are unique'\n",
    "      ).format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of speech\n",
    "- Tokens within each sentence are coded with the parts of speech they play\n",
    "- Useful for distinguishing between homographs (words with same spelling but different meaning)\n",
    "- Polysemy: umbrella term for this kind of linguistic feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(nlp('I need a break')[3].pos_)\n",
    "print(nlp('I need to break the glass')[3].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "parts of speech:\n",
      "There ADV\n",
      "was VERB\n",
      "nothing NOUN\n",
      "so ADV\n",
      "VERY ADV\n",
      "remarkable ADJ\n",
      "in ADP\n",
      "that DET\n",
      "; PUNCT\n"
     ]
    }
   ],
   "source": [
    "#view parts of speach for some tokens in sentence\n",
    "print('\\nparts of speech:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "- How words related to each other syntactically\n",
    "- [Stanford group dependencies page](https://nlp.stanford.edu/software/stanford-dependencies.shtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dependencies:\n",
      "There expl was\n",
      "was ROOT was\n",
      "nothing attr was\n",
      "so advmod remarkable\n",
      "VERY advmod remarkable\n",
      "remarkable amod nothing\n",
      "in prep nothing\n",
      "that pobj in\n",
      "; punct was\n"
     ]
    }
   ],
   "source": [
    "#view dependencies for some tokens\n",
    "print('dependencies:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.dep_, token.head.orth_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities\n",
    "- Some errors: unless an obvious rule applied, spaCy id rules assume that any word/phrase in all caps is an organization or a event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Alice\n",
      "DATE the hot day\n",
      "PERSON Alice\n",
      "PRODUCT Rabbit\n",
      "PRODUCT Rabbit\n",
      "PRODUCT WAISTCOAT - POCKET\n",
      "PERSON Alice\n",
      "PERSON Alice\n",
      "PERSON Alice\n",
      "ORDINAL First\n"
     ]
    }
   ],
   "source": [
    "#extract first ten entities\n",
    "entities = list(alice_doc.ents)[0:10]\n",
    "for entity in entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mock Turtle', 'the Lobster Quadrille?', 'Shakespeare', 'the March Hare', 'Gryphon', 'Morcar', 'Stupid', 'HAD', 'Repeat', 'The Fish-Footman', 'Rabbit', 'Jack', 'Panther', 'Soo', 'M--', 'Longitude', 'Alice', 'Elsie', 'this:--', 'Soles', 'Fury', 'Duchess', 'Mabel', 'Majesty', 'The White Rabbit', 'Pinch', 'the White Rabbit', 'Seaography', 'indeed:--', 'The Queen', 'Edwin', 'Turn', 'Sha', 'Beau', 'Edgar Atheling', 'Fifteenth', '--or', 'Kings', 'Queen', 'Shy', 'Frog-Footman', 'Duck', 'Down', 'Curiouser', 'Cheshire Puss', \"the Duchess: '\", 'Tillie', 'FUL SOUP', 'the Lobster Quadrille', 'Latin Grammar', 'Shall', 'Drink', 'Ma', 'Beautiful Soup', 'Tut', 'INSIDE', 'Hush', 'Treacle', 'Stolen', 'Latitude', 'Mary Ann', 'Footman', 'The Mock Turtle', 'the Duchess', 'Run', 'Canary', 'the King', 'a Lobster Quadrille', 'William', \"the Mock Turtle: '\", 'Hjckrrh', 'Sixteenth', 'Soup of the evening', 'WILLIAM', 'Stand', 'Lacie', 'm--', 'Idiot', 'Fetch', 'began:--', 'Ou', 'William the Conqueror', 'Sentence', 'the Mock Turtle', 'Serpent', 'Begin', 'YOURS', 'Crab', 'Turtle Soup', \"the King: '\", 'Fish-Footman', 'Bill', 'Pat', \"Dinah'll\", 'Soup', 'Brandy', 'Said', 'the Queen of Hearts', 'Adventures', \"Don't\", 'Prizes'}\n"
     ]
    }
   ],
   "source": [
    "#all of the unique entities spaCy identifies as people\n",
    "people = [entity.text for entity in list(alice_doc.ents) if entity.label_ == 'PERSON']\n",
    "print(set(people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
